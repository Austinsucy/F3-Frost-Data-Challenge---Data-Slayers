{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d402b06f-979e-43cf-bb1f-5d6ceadbca2a",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Fetch and Clean Dataset: F3 Innovate Frost Risk Forecasting\n",
    "- **Converted File to Df** so I could use data\n",
    "- **Dictionary of Name -> Data** such that each station name maps to it's historic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c302c-a255-4683-a149-96e142d5f58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas # If Pandas is not already Installed\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35901902-9878-4c4f-bbcb-f878163fe200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "DATA_DIR = \"../cimis-hourly-data-multiple-stations/\"\n",
    "def get_datasets() -> dict: \n",
    "    \"\"\"\n",
    "        Get Dictionary of All (Key (Station-Name), Value (DataFrame)) pairs\n",
    "    \"\"\"\n",
    "    !touch buffer.tmp\n",
    "    !(ls $DATA_DIR | grep \"csv\") > buffer.tmp\n",
    "    name_to_data = dict() # Dictionary for Name to Data\n",
    "    # Read the buffer.tmp and create Name to Data pairs\n",
    "    with open(\"buffer.tmp\", \"r\") as file:\n",
    "        contents = [f for f in file.read().split(\"\\n\") if f]\n",
    "        for content in contents:\n",
    "            file_name = content.split(\".\")[0]\n",
    "            if \"all\" not in file_name:\n",
    "                name_to_data[file_name] = pd.read_csv(DATA_DIR + content)\n",
    "            \n",
    "        !rm -f buffer.tmp\n",
    "        return name_to_data\n",
    "\n",
    "def clean_datasets(datasets: dict) -> dict:\n",
    "    \"\"\"\n",
    "        Clean the Dictionary of Datasets\n",
    "    \"\"\"\n",
    "    for _, df in datasets.items():\n",
    "        df.drop(columns=[col for col in df.columns if \"qc\" in col], inplace=True)\n",
    "        df.rename(columns=lambda c: c.replace(\" \", \"_\"), inplace=True)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31158d6e-da0e-456d-8264-3f4fb709d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = clean_datasets(get_datasets()) # Grabbed and Cleaned Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107469b-fe2a-4aff-ac13-7eb4201bf195",
   "metadata": {},
   "source": [
    "# ðŸ”¦ LightGBM Integration: Calibrated Frost Probabilities + Temperature Forecasts\n",
    "- **Calibrated classification** models for frost within 3/6/12/24 hours (Isotonic).\n",
    "- **Regression** models (and optional quantiles) for temperature at +H hours.\n",
    "- Utilities that respect time ordering (no leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If LightGBM isn't installed in your environment, uncomment and run:\n",
    "!pip install lightgbm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912d09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    brier_score_loss, roc_auc_score, average_precision_score, \n",
    "    mean_absolute_error, confusion_matrix\n",
    ")\n",
    "import lightgbm as lgb\n",
    "\n",
    "def make_features(df, H):\n",
    "    \"\"\"\n",
    "    Make new Features for the Model\n",
    "    NOTE: Spaces (\" \") in Original Features are Replaced with underscores (\"_\")\n",
    "    \n",
    "    Compromise behavior:\n",
    "      - If df has a single station -> use plain rolling/diff (no groupby).\n",
    "      - If df has multiple stations -> use groupby(\"Stn_Id\") for rolling/diff.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --------- Detect single-station vs multi-station mode ---------\n",
    "    has_stn = \"Stn_Id\" in df.columns\n",
    "    n_stations = df[\"Stn_Id\"].nunique() if has_stn else 1\n",
    "    multi_station = has_stn and n_stations > 1\n",
    "\n",
    "    if multi_station:\n",
    "        # Multi-station: groupby\n",
    "        g = df.groupby(\"Stn_Id\", group_keys=False)\n",
    "\n",
    "        def roll(col, w):\n",
    "            return (\n",
    "                g[col]\n",
    "                .rolling(w)\n",
    "                .min()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "        def diff(col, w):\n",
    "            return g[col].diff(w)\n",
    "\n",
    "    else:\n",
    "        # Single-station: no groupby needed\n",
    "        \n",
    "        def roll(col, w):\n",
    "            return df[col].rolling(w).min()\n",
    "\n",
    "        def diff(col, w):\n",
    "            return df[col].diff(w)\n",
    "\n",
    "    # ---------- Simple per-row features (no groupby needed) ----------\n",
    "    # Dewpoint depression: closer = more humid\n",
    "    df[\"dewpoint_dep\"] = df[\"Air_Temp_(C)\"] - df[\"Dew_Point_(C)\"]\n",
    "\n",
    "    # Is dim sunlight\n",
    "    df[\"is_dim\"] = (df[\"Sol_Rad_(W/sq.m)\"] < 50).astype(int)\n",
    "\n",
    "    # Is very calm winds\n",
    "    df[\"is_very_calm\"] = (df[\"Wind_Speed_(m/s)\"] < 1.0).astype(int)\n",
    "\n",
    "    # Is winter based on Julian day\n",
    "    jd = pd.to_numeric(df[\"Jul\"], errors=\"coerce\").fillna(0)  # Julian Day\n",
    "    WINTER_START = 305   # Nov 1\n",
    "    WINTER_END   =  74   # Mar 15\n",
    "    df[\"is_winter\"] = ((jd >= WINTER_START) | (jd <= WINTER_END)).astype(int)\n",
    "\n",
    "    # Hour/day cyclical encodings\n",
    "    hour = pd.to_numeric(df[\"Hour_(PST)\"], errors=\"coerce\").fillna(0) % 24\n",
    "    hour_angle = 2 * np.pi * hour / 24.0\n",
    "\n",
    "    day = pd.to_numeric(df[\"Jul\"], errors=\"coerce\").fillna(0) % 365\n",
    "    day_angle = 2 * np.pi * day / 365.0\n",
    "\n",
    "    df[\"hour_sin\"] = np.sin(hour_angle)\n",
    "    df[\"hour_cos\"] = np.cos(hour_angle)\n",
    "    df[\"day_sin\"] = np.sin(day_angle)\n",
    "    df[\"day_cos\"] = np.cos(day_angle)\n",
    "\n",
    "    # ---------- Time-series features (roll/diff via helpers) ----------\n",
    "    tfs = [3, 6, 12, 24]\n",
    "    for time in tfs:\n",
    "        # Air Temperature\n",
    "        df[f\"air_temp_roll_min_{time}h\"] = roll(\"Air_Temp_(C)\", time)\n",
    "        df[f\"air_temp_change_{time}h\"]   = diff(\"Air_Temp_(C)\", time)\n",
    "\n",
    "        # Dew Point\n",
    "        df[f\"dew_point_roll_min_{time}h\"] = roll(\"Dew_Point_(C)\", time)\n",
    "        df[f\"dew_point_change_{time}h\"]   = diff(\"Dew_Point_(C)\", time)\n",
    "\n",
    "        # Wind Speed\n",
    "        df[f\"wind_spd_roll_min_{time}h\"] = roll(\"Wind_Speed_(m/s)\", time)\n",
    "        df[f\"wind_spd_change_{time}h\"]   = diff(\"Wind_Speed_(m/s)\", time)\n",
    "\n",
    "        # Solar Radiation\n",
    "        df[f\"sol_rad_roll_min_{time}h\"] = roll(\"Sol_Rad_(W/sq.m)\", time)\n",
    "        df[f\"sol_rad_change_{time}h\"]   = diff(\"Sol_Rad_(W/sq.m)\", time)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_targets(df, H):\n",
    "    \"\"\"\n",
    "        Make Targets for the Model\n",
    "        NOTE: Spaces (\" \") in Original Features are Replaced with underscores (\"_\")\n",
    "    \"\"\"\n",
    "    df[f\"frost_{H}h\"] = (df[\"Air_Temp_(C)\"].shift(-H) < 0).astype(int)\n",
    "    df[f\"temp_{H}h\"]  =  df[\"Air_Temp_(C)\"].shift(-H)\n",
    "    return df\n",
    "\n",
    "def build_xy(df_raw, H, feature_cols=None):\n",
    "    \"\"\"\n",
    "        Build the X (Features) and Y (Targets) for the Model\n",
    "    \"\"\"\n",
    "    # Build rebuild df\n",
    "    df = df_raw.copy()\n",
    "    df = make_features(df, H)\n",
    "    df = add_targets(df, H)\n",
    "    \n",
    "    # Targets\n",
    "    frost_col = f\"frost_{H}h\"\n",
    "    temp_col  = f\"temp_{H}h\"\n",
    "    # Columns to Ignore\n",
    "    ignore_cols = ['Stn_Id','Stn_Name','CIMIS_Region', 'Date', 'Hour_(PST)', 'Jul']\n",
    "\n",
    "    # Filter for Feature Cols\n",
    "    if feature_cols is None:\n",
    "        is_valid_feature_col = lambda c: c in df.columns and c not in [frost_col, temp_col, *ignore_cols]\n",
    "        feature_cols = [c for c in df.columns if is_valid_feature_col(c)]\n",
    "\n",
    "    # X (Features) and Y (Targets)\n",
    "    needed = feature_cols + [frost_col, temp_col]\n",
    "    df = df.dropna(subset=[c for c in needed if c in df.columns])\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y_frost = df[frost_col].astype(int)\n",
    "    y_temp  = df[temp_col].astype(float)\n",
    "    return X, y_frost, y_temp, feature_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e6d0db-53da-414c-897f-1b378f49d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compute_and_plot_feature_corr(df_raw, H=6, feature_cols=None, figsize=(10,8)):\n",
    "    \"\"\"\n",
    "    Build X using your pipeline, compute featureâ†”feature correlation matrix,\n",
    "    and plot it using matplotlib.\n",
    "    \"\"\"\n",
    "    # Build features only (no targets in matrix)\n",
    "    X, _, _, used_features = build_xy(df_raw, H, feature_cols=feature_cols)\n",
    "    # Correlation matrix\n",
    "    corr = X.corr(method=\"pearson\")\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(corr, cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar(label='Correlation')\n",
    "\n",
    "    # Tick labels\n",
    "    plt.xticks(ticks=np.arange(len(used_features)), labels=used_features, rotation=90)\n",
    "    plt.yticks(ticks=np.arange(len(used_features)), labels=used_features)\n",
    "\n",
    "    plt.title(f\"Feature â†” Feature Correlation Matrix (H={H}h)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return corr\n",
    "\n",
    "name = \"80-fresnostate\"\n",
    "df = datasets[name]\n",
    "#compute_and_plot_feature_corr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbae828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, H=6, threshold=0.5, verbose=True):\n",
    "    # Build features/targets with your existing helpers\n",
    "    X, y_frost, y_temp, feat_cols = build_xy(df, H)  # uses ensure_timestamp -> make_features -> add_targets\n",
    "\n",
    "    # Time-ordered split: 70% train, 15% calibration, 15% test\n",
    "    n = len(X)\n",
    "    i_tr, i_cal = int(n * 0.70), int(n * 0.85)\n",
    "\n",
    "    X_tr, y_tr = X.iloc[:i_tr], y_frost.iloc[:i_tr]\n",
    "    X_cal, y_cal = X.iloc[i_tr:i_cal], y_frost.iloc[i_tr:i_cal]\n",
    "    X_te,  y_te  = X.iloc[i_cal:],      y_frost.iloc[i_cal:]\n",
    "    y_temp_trcal = y_temp.iloc[:i_cal]\n",
    "    y_temp_te    = y_temp.iloc[i_cal:]\n",
    "\n",
    "    # --- Classifier ---\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.002,\n",
    "        class_weight=\"balanced\",\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # Probability calibration on the calibration window (avoids cv='prefit' deprecation)\n",
    "    calib = CalibratedClassifierCV(estimator=clf, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_cal, y_cal)\n",
    "    p_te = calib.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    # --- Turn probabilities into hard predictions at the chosen threshold ---\n",
    "    y_pred = (p_te >= threshold).astype(int)\n",
    "\n",
    "    # Confusion matrix: TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "\n",
    "    # Derived stats\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr       = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    tnr       = tn / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    # --- Regressor (train on train+cal) ---\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42\n",
    "    )\n",
    "    reg.fit(pd.concat([X_tr, X_cal]), y_temp_trcal)\n",
    "    yhat_te = reg.predict(X_te)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    metrics = {\n",
    "        \"AUROC\":    float(roc_auc_score(y_te, p_te)),\n",
    "        \"AUPRC\":    float(average_precision_score(y_te, p_te)),\n",
    "        \"Brier\":    float(brier_score_loss(y_te, p_te)),\n",
    "        \"MAE_temp\": float(mean_absolute_error(y_temp_te, yhat_te)),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"TP\": int(tp),\n",
    "        \"FP\": int(fp),\n",
    "        \"TN\": int(tn),\n",
    "        \"FN\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TNR\": float(tnr),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"H={H}h | AUROC={metrics['AUROC']:.3f}  AUPRC={metrics['AUPRC']:.3f}  \"\n",
    "            f\"Brier={metrics['Brier']:.3f}  MAE_temp={metrics['MAE_temp']:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"thr={threshold:.2f} | TP={tp} FP={fp} TN={tn} FN={fn} \"\n",
    "            f\"(precision={precision:.3f}, recall={recall:.3f}, FPR={fpr:.3f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"There is a {p_te[-1]*100:.2f}% chance of frost in the next {H} hours, \"\n",
    "            f\"predicted temperature: {yhat_te[-1]:.2f} Â°C\"\n",
    "        )\n",
    "\n",
    "    return calib, reg, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c3296-9570-4a6b-9a37-e3651ccc6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"80-fresnostate\" # Station Name\n",
    "df = datasets[name] # DataSet\n",
    "tfs = [3, 6, 12, 24] # TimeFrames\n",
    "# For Each TimeFrame Train/Test a Model\n",
    "for tf in tfs:\n",
    "    _calib, _reg, _metrics = train_model(df, H=tf, threshold=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e9b8a-87c5-48b1-836e-5d2314bf424b",
   "metadata": {},
   "source": [
    "# ðŸ¤ Teacher Ensemble Distillation: Unified Student Frost & Temperature Model\n",
    "- **Weighted teacher ensemble** using accuracyâ€“correlation scores (acc / Î£ corrÂ² â†’ softmax).\n",
    "- **Student classification** model trained from blended hard labels + teacher soft probabilities.\n",
    "- **Student regression** model distilled from ensemble temperature forecasts.\n",
    "- Enables a **global multi-station model** with improved generalization across microclimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67600392-d081-4785-a1bd-ceb85c1d3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_teacher_weights(teachers, X_ref, y_ref, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute teacher weights:\n",
    "\n",
    "        score_i = acc_i / sum_j corr(i, j)^2\n",
    "        w_i = softmax(score_i)\n",
    "\n",
    "    where:\n",
    "        acc_i = accuracy of teacher i on (X_ref, y_ref)\n",
    "        corr(i, j) = Pearson correlation between teacher i and j's probabilities\n",
    "                     (using raw correlation, then squared before summing).\n",
    "    \"\"\"\n",
    "    n_teachers = len(teachers)\n",
    "    if n_teachers == 1:\n",
    "        return np.array([1.0], dtype=float)\n",
    "\n",
    "    # (n_samples, n_teachers) matrix of predicted probabilities for class 1 (frost)\n",
    "    preds = np.column_stack([t.predict_proba(X_ref)[:, 1] for t in teachers])\n",
    "\n",
    "    # accuracy per teacher at threshold 0.5\n",
    "    hard_preds = (preds >= 0.5).astype(int)          # (n_samples, n_teachers)\n",
    "    y_ref_arr = y_ref.values.reshape(-1, 1)          # (n_samples, 1)\n",
    "    acc = (hard_preds == y_ref_arr).mean(axis=0)     # (n_teachers,)\n",
    "\n",
    "    # Pearson correlation matrix between teacher outputs\n",
    "    # corr_matrix[i, j] = corr(preds[:, i], preds[:, j])\n",
    "    corr_matrix = np.corrcoef(preds, rowvar=False)   # (n_teachers, n_teachers)\n",
    "\n",
    "    # we don't want self-correlation; set diagonal to 0 so it doesn't affect the sum\n",
    "    np.fill_diagonal(corr_matrix, 0.0)\n",
    "\n",
    "    # sum of squared correlations to *other* teachers\n",
    "    # denom_i = Î£_j corr(i, j)^2\n",
    "    sum_sq_corr = np.sum(corr_matrix ** 2, axis=1)   # (n_teachers,)\n",
    "\n",
    "    # score = acc / (sum of squared correlations)\n",
    "    score = acc / (sum_sq_corr + eps)\n",
    "\n",
    "    # softmax normalization\n",
    "    score_shifted = score - score.max()              # for numerical stability\n",
    "    exp_score = np.exp(score_shifted)\n",
    "    weights = exp_score / exp_score.sum()            # (n_teachers,)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _ensemble_probs(teachers, X, weights=None):\n",
    "    \"\"\"Compute ensemble frost probability for each row in X.\"\"\"\n",
    "    preds = np.column_stack([t.predict_proba(X)[:, 1] for t in teachers])  # (n_samples, n_teachers)\n",
    "\n",
    "    if weights is None:\n",
    "        # equal weights if none provided\n",
    "        w = np.ones(preds.shape[1], dtype=float) / preds.shape[1]\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=float)\n",
    "        w = w / w.sum()\n",
    "\n",
    "    return preds @ w  # (n_samples, n_teachers) @ (n_teachers,) -> (n_samples,)\n",
    "\n",
    "def _ensemble_reg_preds(regressors, X, weights=None):\n",
    "    \"\"\"Ensemble temperature predictions from regression teachers.\"\"\"\n",
    "    preds = np.column_stack([r.predict(X) for r in regressors])\n",
    "    # preds shape: (n_samples, n_regressors)\n",
    "\n",
    "    if weights is None:\n",
    "        w = np.ones(preds.shape[1], dtype=float) / preds.shape[1]\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=float)\n",
    "        w = w / w.sum()\n",
    "\n",
    "    return preds @ w  # (n_samples, n_regressors) @ (n_regressors,) -> (n_samples,)\n",
    "\n",
    "def train_student_from_ensemble(\n",
    "    df,\n",
    "    teachers,\n",
    "    reg_teachers,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    teacher_weights=None,   # if None, computed via acc / sum corr^2 on calibration set\n",
    "    alpha=0.7,              # KD blend for classification\n",
    "    beta=0.7,               # KD blend for regression\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a student LightGBM 'classifier' (as regressor on [0,1]) + regressor by distilling knowledge\n",
    "    from ensembles of teacher classifiers and teacher regressors.\n",
    "\n",
    "    teachers: list of fitted classifiers with predict_proba\n",
    "    reg_teachers: list of fitted regressors with predict (same ordering as teachers)\n",
    "    teacher_weights: optional list of weights; if None, computed from classifier teachers.\n",
    "    alpha: blend for classification KD (1.0 -> pure ground truth, 0.0 -> pure teacher)\n",
    "    beta:  blend for regression KD (1.0 -> pure ground truth, 0.0 -> pure teacher)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Build features/targets as in your original pipeline ---\n",
    "    X, y_frost, y_temp, feat_cols = build_xy(df, H)\n",
    "\n",
    "    # Time-ordered split: 70% train, 15% calibration, 15% test\n",
    "    n = len(X)\n",
    "    i_tr, i_cal = int(n * 0.70), int(n * 0.85)\n",
    "\n",
    "    X_tr, y_tr = X.iloc[:i_tr],        y_frost.iloc[:i_tr]\n",
    "    X_cal, y_cal = X.iloc[i_tr:i_cal], y_frost.iloc[i_tr:i_cal]\n",
    "    X_te,  y_te  = X.iloc[i_cal:],     y_frost.iloc[i_cal:]\n",
    "    y_temp_trcal = y_temp.iloc[:i_cal]   # for regressor: train + cal\n",
    "    y_temp_te    = y_temp.iloc[i_cal:]\n",
    "\n",
    "    X_trcal = pd.concat([X_tr, X_cal])    # for regression KD features\n",
    "\n",
    "    # ---------- 1) If no weights are provided, compute them on the calibration slice ----------\n",
    "    if teacher_weights is None:\n",
    "        teacher_weights = _compute_teacher_weights(teachers, X_cal, y_cal)\n",
    "        if verbose:\n",
    "            print(\"Teacher weights (acc / sum corr^2 â†’ softmax):\", teacher_weights)\n",
    "\n",
    "    # ---------- 2) Classification KD: ensemble teacher probabilities ----------\n",
    "    p_teacher_tr  = _ensemble_probs(teachers, X_tr,  teacher_weights)\n",
    "    p_teacher_cal = _ensemble_probs(teachers, X_cal, teacher_weights)  # can be used later if you want calibration\n",
    "\n",
    "    # KD targets for classification: blend hard labels + teacher soft probs\n",
    "    # y_tr in {0,1}, p_teacher_tr in [0,1]\n",
    "    y_tr_kd = alpha * y_tr.values + (1.0 - alpha) * p_teacher_tr\n",
    "\n",
    "    # ---------- 3) Train student 'classifier' as a regressor on KD labels ----------\n",
    "    # This model outputs a continuous value in [~0,1] which we interpret as frost probability.\n",
    "    student_clf = lgb.LGBMRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.002,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "    )\n",
    "    student_clf.fit(X_tr, y_tr_kd)\n",
    "\n",
    "    # Student test \"probabilities\" (regression output, clipped to [0,1])\n",
    "    p_te_student = student_clf.predict(X_te)\n",
    "    p_te_student = np.clip(p_te_student, 0.0, 1.0)\n",
    "\n",
    "    # Hard predictions at chosen threshold\n",
    "    y_pred = (p_te_student >= threshold).astype(int)\n",
    "\n",
    "    # ---------- 4) Regression KD: ensemble teacher temperature predictions ----------\n",
    "    # Teacher temp ensemble for train+cal window\n",
    "    temp_teacher_trcal = _ensemble_reg_preds(reg_teachers, X_trcal, teacher_weights)\n",
    "\n",
    "    # KD targets: blend hard temps + teacher temps\n",
    "    y_temp_trcal_kd = beta * y_temp_trcal.values + (1.0 - beta) * temp_teacher_trcal\n",
    "\n",
    "    # Train student regressor on KD temps\n",
    "    student_reg = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "    )\n",
    "    student_reg.fit(X_trcal, y_temp_trcal_kd)\n",
    "\n",
    "    # Student temp predictions on test\n",
    "    yhat_te = student_reg.predict(X_te)\n",
    "\n",
    "    # ---------- 5) Metrics ----------\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr       = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    tnr       = tn / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"AUROC\":    float(roc_auc_score(y_te, p_te_student)),\n",
    "        \"AUPRC\":    float(average_precision_score(y_te, p_te_student)),\n",
    "        \"Brier\":    float(brier_score_loss(y_te, p_te_student)),\n",
    "        \"MAE_temp\": float(mean_absolute_error(y_temp_te, yhat_te)),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"TP\": int(tp),\n",
    "        \"FP\": int(fp),\n",
    "        \"TN\": int(tn),\n",
    "        \"FN\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TNR\": float(tnr),\n",
    "        \"alpha\": float(alpha),\n",
    "        \"beta\": float(beta),\n",
    "        \"teacher_weights\": teacher_weights.tolist(),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[STUDENT] H={H}h | AUROC={metrics['AUROC']:.3f}  \"\n",
    "            f\"AUPRC={metrics['AUPRC']:.3f}  Brier={metrics['Brier']:.3f}  \"\n",
    "            f\"MAE_temp={metrics['MAE_temp']:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"thr={threshold:.2f} | TP={tp} FP={fp} TN={tn} FN={fn} \"\n",
    "            f\"(precision={precision:.3f}, recall={recall:.3f}, FPR={fpr:.3f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"There is a {p_te_student[-1]*100:.2f}% chance of frost in the next {H} hours, \"\n",
    "            f\"predicted temperature: {yhat_te[-1]:.2f} Â°C\"\n",
    "        )\n",
    "\n",
    "    return student_clf, student_reg, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23dcdd69-1887-4ec2-af98-8317a44043fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 105-westlands...\n",
      "H=3h | AUROC=0.989  AUPRC=0.806  Brier=0.004  MAE_temp=0.99\n",
      "thr=0.50 | TP=149 FP=64 TN=19237 FN=36 (precision=0.700, recall=0.805, FPR=0.003)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 19.73 Â°C\n",
      "\n",
      "Training 124-panoche...\n",
      "H=3h | AUROC=0.980  AUPRC=0.642  Brier=0.003  MAE_temp=1.09\n",
      "thr=0.50 | TP=64 FP=60 TN=18092 FN=19 (precision=0.516, recall=0.771, FPR=0.003)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 17.96 Â°C\n",
      "\n",
      "Training 125-arvinedison...\n",
      "H=3h | AUROC=0.998  AUPRC=0.590  Brier=0.002  MAE_temp=1.07\n",
      "thr=0.50 | TP=4 FP=2 TN=18448 FN=64 (precision=0.667, recall=0.059, FPR=0.000)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 19.82 Â°C\n",
      "\n",
      "Training 131-fairoaks...\n",
      "H=3h | AUROC=0.999  AUPRC=0.768  Brier=0.001  MAE_temp=0.99\n",
      "thr=0.50 | TP=23 FP=7 TN=19283 FN=9 (precision=0.767, recall=0.719, FPR=0.000)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 16.35 Â°C\n",
      "\n",
      "Training 146-belridge...\n",
      "H=3h | AUROC=0.999  AUPRC=0.716  Brier=0.002  MAE_temp=1.03\n",
      "thr=0.50 | TP=35 FP=30 TN=19006 FN=12 (precision=0.538, recall=0.745, FPR=0.002)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 17.24 Â°C\n",
      "\n",
      "Training 15-stratford...\n",
      "H=3h | AUROC=0.998  AUPRC=0.776  Brier=0.003  MAE_temp=0.91\n",
      "thr=0.50 | TP=100 FP=35 TN=19335 FN=36 (precision=0.741, recall=0.735, FPR=0.002)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 19.20 Â°C\n",
      "\n",
      "Training 182-delano...\n",
      "H=3h | AUROC=0.998  AUPRC=0.750  Brier=0.004  MAE_temp=1.09\n",
      "thr=0.50 | TP=104 FP=30 TN=18472 FN=41 (precision=0.776, recall=0.717, FPR=0.002)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 19.36 Â°C\n",
      "\n",
      "Training 194-oakdale...\n",
      "H=3h | AUROC=0.997  AUPRC=0.583  Brier=0.004  MAE_temp=0.87\n",
      "thr=0.50 | TP=72 FP=67 TN=18561 FN=20 (precision=0.518, recall=0.783, FPR=0.004)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 16.41 Â°C\n",
      "\n",
      "Training 195-auburn...\n",
      "H=3h | AUROC=0.998  AUPRC=0.096  Brier=0.001  MAE_temp=1.02\n",
      "thr=0.50 | TP=0 FP=8 TN=19337 FN=5 (precision=0.000, recall=0.000, FPR=0.000)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 13.63 Â°C\n",
      "\n",
      "Training 2-fivepoints...\n",
      "H=3h | AUROC=0.990  AUPRC=0.713  Brier=0.003  MAE_temp=0.93\n",
      "thr=0.50 | TP=52 FP=11 TN=19168 FN=70 (precision=0.825, recall=0.426, FPR=0.001)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 18.71 Â°C\n",
      "\n",
      "Training 205-coalinga...\n",
      "H=3h | AUROC=0.991  AUPRC=0.165  Brier=0.003  MAE_temp=1.01\n",
      "thr=0.50 | TP=0 FP=0 TN=15181 FN=22 (precision=0.000, recall=0.000, FPR=0.000)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 33.25 Â°C\n",
      "\n",
      "Training 206-denairII...\n",
      "H=3h | AUROC=0.998  AUPRC=0.660  Brier=0.004  MAE_temp=0.82\n",
      "thr=0.50 | TP=76 FP=79 TN=18228 FN=10 (precision=0.490, recall=0.884, FPR=0.004)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 16.93 Â°C\n",
      "\n",
      "Training 39-parlier...\n",
      "H=3h | AUROC=0.990  AUPRC=0.583  Brier=0.002  MAE_temp=0.86\n",
      "thr=0.50 | TP=45 FP=37 TN=19307 FN=20 (precision=0.549, recall=0.692, FPR=0.002)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 19.30 Â°C\n",
      "\n",
      "Training 47-brentwood...\n",
      "H=3h | AUROC=0.974  AUPRC=0.429  Brier=0.001  MAE_temp=1.09\n",
      "thr=0.50 | TP=13 FP=19 TN=18495 FN=9 (precision=0.406, recall=0.591, FPR=0.001)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 17.83 Â°C\n",
      "\n",
      "Training 7-firebaugh...\n",
      "H=3h | AUROC=0.997  AUPRC=0.269  Brier=0.002  MAE_temp=0.91\n",
      "thr=0.50 | TP=6 FP=15 TN=15429 FN=20 (precision=0.286, recall=0.231, FPR=0.001)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 19.52 Â°C\n",
      "\n",
      "Training 70-manteca...\n",
      "H=3h | AUROC=0.988  AUPRC=0.640  Brier=0.003  MAE_temp=0.95\n",
      "thr=0.50 | TP=76 FP=50 TN=18781 FN=22 (precision=0.603, recall=0.776, FPR=0.003)\n",
      "There is a 0.03% chance of frost in the next 3 hours, predicted temperature: 16.34 Â°C\n",
      "\n",
      "Training 71-modesto...\n",
      "H=3h | AUROC=0.997  AUPRC=0.869  Brier=0.005  MAE_temp=0.99\n",
      "thr=0.50 | TP=225 FP=89 TN=18975 FN=30 (precision=0.717, recall=0.882, FPR=0.005)\n",
      "There is a 0.02% chance of frost in the next 3 hours, predicted temperature: 15.12 Â°C\n",
      "\n",
      "Training 80-fresnostate...\n",
      "H=3h | AUROC=0.999  AUPRC=0.711  Brier=0.002  MAE_temp=0.85\n",
      "thr=0.50 | TP=64 FP=29 TN=18934 FN=21 (precision=0.688, recall=0.753, FPR=0.002)\n",
      "There is a 0.01% chance of frost in the next 3 hours, predicted temperature: 19.97 Â°C\n",
      "\n",
      "Training Student from Teachers...\n",
      "Teacher weights (acc / sum corr^2 â†’ softmax): [0.05507952 0.05515765 0.05550453 0.0552628  0.05508352 0.05508923\n",
      " 0.05509081 0.0554354  0.05625451 0.05510787 0.05780588 0.05539361\n",
      " 0.05556309 0.05569733 0.05522859 0.05524443 0.05566265 0.05633856]\n",
      "[STUDENT] H=3h | AUROC=0.996  AUPRC=0.862  Brier=0.004  MAE_temp=1.02\n",
      "thr=0.50 | TP=2185 FP=242 TN=330468 FN=1579 (precision=0.900, recall=0.580, FPR=0.001)\n",
      "There is a 0.23% chance of frost in the next 3 hours, predicted temperature: 20.13 Â°C\n"
     ]
    }
   ],
   "source": [
    "tfs = [3,] # TimeFrames\n",
    "teachers = [] # Teachers (Classificaion)\n",
    "reg_teachers = [] # Teachers (Regression)\n",
    "stations = list(datasets.keys()) # Stations\n",
    "df_all = pd.concat(list(datasets.values()), ignore_index=True) # DataFrame with All Stations\n",
    "for tf in tfs:\n",
    "    for station in stations:\n",
    "        print(f\"\\nTraining {station}...\" )\n",
    "        calib, reg, _ = train_model(datasets[station], tf, threshold=0.5, verbose=True)\n",
    "        teachers.append(calib); reg_teachers.append(reg)\n",
    "    print(f\"\\nTraining Student from Teachers...\")\n",
    "    calib_student, student_reg, metrics = (\n",
    "        # Alpha and Beta are best at 0.7\n",
    "        train_student_from_ensemble(df_all, teachers, reg_teachers, tf, \n",
    "                                    threshold=0.5,alpha=0.7, beta=0.7, verbose=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0d4fab2-7e3a-4f20-a0fb-bb19de5e8601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training General Model...\n",
      "H=3h | AUROC=0.995  AUPRC=0.714  Brier=0.005  MAE_temp=1.00\n",
      "thr=0.50 | TP=2703 FP=1160 TN=329550 FN=1061 (precision=0.700, recall=0.718, FPR=0.004)\n",
      "There is a 0.00% chance of frost in the next 3 hours, predicted temperature: 20.29 Â°C\n"
     ]
    }
   ],
   "source": [
    "tf = 3 # TimeFrames\n",
    "stations = list(datasets.keys()) # Stations\n",
    "df_all = pd.concat(list(datasets.values()), ignore_index=True) # DataFrame with All Stations\n",
    "print(f\"\\n Training General Model...\")\n",
    "calib, reg, _ = train_model(df_all, tf, threshold=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d7e0b90-0c38-4899-85dd-fc6dc658f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"new_models\"\n",
    "# Create directory for saving\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save teachers\n",
    "for i, (clf, reg) in enumerate(zip(teachers, reg_teachers)):\n",
    "    joblib.dump(clf, f\"{MODEL_DIR}/teacher_clf_{i}.pkl\")\n",
    "    joblib.dump(reg, f\"{MODEL_DIR}/teacher_reg_{i}.pkl\")\n",
    "\n",
    "# Save student models\n",
    "joblib.dump(calib_student, f\"{MODEL_DIR}/student_clf.pkl\")\n",
    "joblib.dump(student_reg, f\"{MODEL_DIR}/student_reg.pkl\")\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d724f-8b0a-4469-9018-506b21836712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "lightgbm_integration": {
   "appended_at": "2025-11-17T03:27:25.431377Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
