{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537c304f",
   "metadata": {},
   "source": [
    "# üìÇ Dataset Loading & Preprocessing\n",
    "\n",
    "In this section, we load all station-level datasets, merge them into a unified structure, clean missing values, convert timestamps, and prepare them for feature engineering.\n",
    "\n",
    "## Topics Covered\n",
    "- Loading CSV files for each station & Assembling a dictionary of datasets (`get_datasets`)\n",
    "- Cleaning and normalizing feature columns (`clean_datasets`)\n",
    "- Feature engineering (`make_features`)\n",
    "- Adding frost/temperature targets (`add_targets`)\n",
    "- Building inputs and outputs for models to fit on (`build_xy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73489da-f14f-47ea-b1c7-e520562d4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    mean_absolute_error,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# Root directory for plots\n",
    "PLOTS_ROOT = Path(\"plots\")\n",
    "PLOTS_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# Root directory for models\n",
    "MODELS_ROOT = Path(\"models\")\n",
    "MODELS_ROOT.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ba97dd-9b51-4c5b-ae9b-b80ecf0cabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path on CIMIS Datasets [Custom]\n",
    "DATA_DIR = \"../cimis-hourly-data-multiple-stations/\"\n",
    "\n",
    "def get_datasets() -> dict: \n",
    "    !touch buffer.tmp\n",
    "    !(ls $DATA_DIR | grep \"csv\") > buffer.tmp\n",
    "    name_to_data = dict() # Dictionary for Name to Data\n",
    "    # Read the buffer.tmp and create Name to Data pairs\n",
    "    with open(\"buffer.tmp\", \"r\") as file:\n",
    "        contents = [f for f in file.read().split(\"\\n\") if f]\n",
    "        for content in contents:\n",
    "            file_name = content.split(\".\")[0]\n",
    "            if \"all\" not in file_name:\n",
    "                name_to_data[file_name] = pd.read_csv(DATA_DIR + content)\n",
    "            \n",
    "        !rm -f buffer.tmp\n",
    "        return name_to_data\n",
    "\n",
    "def clean_datasets(datasets: dict) -> dict:\n",
    "    for _, df in datasets.items():\n",
    "        df.drop(columns=[col for col in df.columns if \"qc\" in col], inplace=True)\n",
    "        df.rename(columns=lambda c: c.replace(\" \", \"_\"), inplace=True)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e6482d-c358-488f-91ff-cd3554af490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df, H):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Detect single-station vs multi-station mode\n",
    "    has_stn = \"Stn_Id\" in df.columns\n",
    "    n_stations = df[\"Stn_Id\"].nunique() if has_stn else 1\n",
    "    multi_station = has_stn and n_stations > 1\n",
    "\n",
    "    if multi_station:\n",
    "        # Multi-station: groupby\n",
    "        g = df.groupby(\"Stn_Id\", group_keys=False)\n",
    "\n",
    "        def roll(col, w):\n",
    "            return (\n",
    "                g[col]\n",
    "                .rolling(w)\n",
    "                .min()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "        def diff(col, w):\n",
    "            return g[col].diff(w)\n",
    "\n",
    "    else:\n",
    "        # Single-station: no groupby needed\n",
    "        \n",
    "        def roll(col, w):\n",
    "            return df[col].rolling(w).min()\n",
    "\n",
    "        def diff(col, w):\n",
    "            return df[col].diff(w)\n",
    "\n",
    "    # ---------- Simple per-row features (no groupby needed) ----------\n",
    "    # Dewpoint depression: closer = more humid\n",
    "    df[\"dewpoint_dep\"] = df[\"Air_Temp_(C)\"] - df[\"Dew_Point_(C)\"]\n",
    "    # Is dim sunlight\n",
    "    df[\"is_dim\"] = (df[\"Sol_Rad_(W/sq.m)\"] < 50).astype(int)\n",
    "    # Is very calm winds\n",
    "    df[\"is_very_calm\"] = (df[\"Wind_Speed_(m/s)\"] < 1.0).astype(int)\n",
    "\n",
    "    # Is winter based on Julian day\n",
    "    jd = pd.to_numeric(df[\"Jul\"], errors=\"coerce\").fillna(0)  # Julian Day\n",
    "    WINTER_START = 305   # Nov 1\n",
    "    WINTER_END   =  74   # Mar 15\n",
    "    df[\"is_winter\"] = ((jd >= WINTER_START) | (jd <= WINTER_END)).astype(int)\n",
    "\n",
    "    # Hour/day cyclical encodings\n",
    "    hour = pd.to_numeric(df[\"Hour_(PST)\"], errors=\"coerce\").fillna(0) % 24\n",
    "    hour_angle = 2 * np.pi * hour / 24.0\n",
    "\n",
    "    day = pd.to_numeric(df[\"Jul\"], errors=\"coerce\").fillna(0) % 365\n",
    "    day_angle = 2 * np.pi * day / 365.0\n",
    "\n",
    "    df[\"hour_sin\"] = np.sin(hour_angle)\n",
    "    df[\"hour_cos\"] = np.cos(hour_angle)\n",
    "    df[\"day_sin\"] = np.sin(day_angle)\n",
    "    df[\"day_cos\"] = np.cos(day_angle)\n",
    "\n",
    "    # ---------- Time-series features (roll/diff via helpers) ----------\n",
    "    tfs = [3, 6, 12, 24]\n",
    "    for time in tfs:\n",
    "        # Air Temperature\n",
    "        df[f\"air_temp_roll_min_{time}h\"] = roll(\"Air_Temp_(C)\", time)\n",
    "        df[f\"air_temp_change_{time}h\"]   = diff(\"Air_Temp_(C)\", time)\n",
    "\n",
    "        # Dew Point\n",
    "        df[f\"dew_point_roll_min_{time}h\"] = roll(\"Dew_Point_(C)\", time)\n",
    "        df[f\"dew_point_change_{time}h\"]   = diff(\"Dew_Point_(C)\", time)\n",
    "\n",
    "        # Wind Speed\n",
    "        df[f\"wind_spd_roll_min_{time}h\"] = roll(\"Wind_Speed_(m/s)\", time)\n",
    "        df[f\"wind_spd_change_{time}h\"]   = diff(\"Wind_Speed_(m/s)\", time)\n",
    "\n",
    "        # Solar Radiation\n",
    "        df[f\"sol_rad_roll_min_{time}h\"] = roll(\"Sol_Rad_(W/sq.m)\", time)\n",
    "        df[f\"sol_rad_change_{time}h\"]   = diff(\"Sol_Rad_(W/sq.m)\", time)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_targets(df, H):\n",
    "    df[f\"frost_{H}h\"] = (df[\"Air_Temp_(C)\"].shift(-H) < 0).astype(int)\n",
    "    df[f\"temp_{H}h\"]  =  df[\"Air_Temp_(C)\"].shift(-H)\n",
    "    return df\n",
    "\n",
    "def build_xy(df_raw, H, feature_cols=None):\n",
    "    # Build rebuild df\n",
    "    df = df_raw.copy()\n",
    "    df = make_features(df, H)\n",
    "    df = add_targets(df, H)\n",
    "    \n",
    "    # Targets\n",
    "    frost_col = f\"frost_{H}h\"\n",
    "    temp_col  = f\"temp_{H}h\"\n",
    "    # Columns to Ignore\n",
    "    ignore_cols = ['Stn_Id','Stn_Name','CIMIS_Region', 'Date', 'Hour_(PST)', 'Jul']\n",
    "\n",
    "    # Filter for Feature Cols\n",
    "    if feature_cols is None:\n",
    "        is_valid_feature_col = lambda c: c in df.columns and c not in [frost_col, temp_col, *ignore_cols]\n",
    "        feature_cols = [c for c in df.columns if is_valid_feature_col(c)]\n",
    "\n",
    "    # X (Features) and Y (Targets)\n",
    "    needed = feature_cols + [frost_col, temp_col]\n",
    "    df = df.dropna(subset=[c for c in needed if c in df.columns])\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y_frost = df[frost_col].astype(int)\n",
    "    y_temp  = df[temp_col].astype(float)\n",
    "    return X, y_frost, y_temp, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac013e-8730-413f-a6ac-7f0a8b0b7ece",
   "metadata": {},
   "source": [
    "# üìä Metrics & Visualization Tools\n",
    "\n",
    "This section introduces the tools used to evaluate calibration, discrimination, and regression performance across both teacher and student models. It includes scalar metrics, curve-based evaluation tools, horizon summaries, and LOSO spatial generalization plots.\n",
    "\n",
    "## Metrics Included\n",
    "- ROC‚ÄìAUC-CURVE ‚Äì discrimination score for ranking  \n",
    "- PR‚ÄìAUC_CUR ‚Äì ranking quality for rare frost events  \n",
    "- Brier Score ‚Äì probability calibration and sharpness  \n",
    "- ECE (Expected Calibration Error) ‚Äì bucketed calibration measure  \n",
    "- MAE (Temperature) ‚Äì regression accuracy for predicted temperature  \n",
    "- Confusion Matrix (TP, FP, TN, FN)\n",
    "\n",
    "---\n",
    "\n",
    "## Visualization & Metric Helper Functions\n",
    "\n",
    "- **plot_roc_curves**  \n",
    "  Plots ROC curves for multiple models using a dictionary of probability outputs and reports AUC for each model.\n",
    "\n",
    "- **plot_pr_curves**  \n",
    "  Plots Precision‚ÄìRecall curves for multiple models and computes Average Precision (AP / PR‚ÄìAUC), useful for rare-event evaluation.\n",
    "\n",
    "- **plot_reliability_curves**  \n",
    "  Generates calibration diagrams by binning predicted probabilities and comparing predicted vs. observed outcome frequencies.\n",
    "\n",
    "- **compute_ece**  \n",
    "  Computes Expected Calibration Error (ECE) by aggregating the weighted accuracy‚Äìconfidence gap over probability bins.\n",
    "\n",
    "- **plot_metric_bar**  \n",
    "  Produces bar charts for any scalar metric across models (e.g., Brier, MAE, ECE) using a `{model_name: value}` dictionary.\n",
    "\n",
    "- **plot_horizon_summaries**  \n",
    "  Plots metric performance across forecasting horizons (e.g., 3h, 6h, 12h, 24h), generating one summary chart per model √ó metric.\n",
    "\n",
    "- **plot_loso_summary**  \n",
    "  Visualizes LOSO cross-station generalization using mean ¬± standard deviation bar charts across metrics (Brier, ECE, ROC‚ÄìAUC, PR‚ÄìAUC, MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d248c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(y_true, prob_dict, title=None, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    for label, probs in prob_dict.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, label=f\"{label} (AUC={roc_auc:.3f})\")\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title or \"ROC curves\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_pr_curves(y_true, prob_dict, title=None, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    for label, probs in prob_dict.items():\n",
    "        precision, recall, _ = precision_recall_curve(y_true, probs)\n",
    "        ap = average_precision_score(y_true, probs)\n",
    "        ax.plot(recall, precision, label=f\"{label} (AP={ap:.3f})\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(title or \"Precision‚ÄìRecall curves\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_reliability_curves(y_true, prob_dict, n_bins=10, title=None, save_path=None):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    for label, probs in prob_dict.items():\n",
    "        prob_true, prob_pred = calibration_curve(\n",
    "            y_true, probs, n_bins=n_bins, strategy=\"uniform\"\n",
    "        )\n",
    "        ax.plot(prob_pred, prob_true, marker=\"o\", linewidth=1, label=label)\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_ylabel(\"Observed frequency\")\n",
    "    ax.set_title(title or \"Reliability diagram\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_metric_bar(metric_name, metric_dict, title=None, save_path=None):\n",
    "    labels = list(metric_dict.keys())\n",
    "    values = [metric_dict[k] for k in labels]\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, values)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=15)\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(title or metric_name)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "def plot_horizon_summaries(metrics_over_H, metrics_to_plot):\n",
    "    for model_name, by_H in metrics_over_H.items():\n",
    "\n",
    "        model_dir = PLOTS_ROOT / str(model_name)\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for metric_name in metrics_to_plot:\n",
    "\n",
    "            # Extract {H: metric_value}\n",
    "            metric_series = {\n",
    "                H: m[metric_name]\n",
    "                for H, m in by_H.items()\n",
    "                if metric_name in m\n",
    "            }\n",
    "\n",
    "            if not metric_series:\n",
    "                continue\n",
    "\n",
    "            # Sort horizons (so bars go in correct order)\n",
    "            horizons = sorted(metric_series.keys())   # e.g. [3, 6, 12, 24]\n",
    "            values   = [metric_series[H] for H in horizons]\n",
    "\n",
    "            # Bar plot\n",
    "            fig, ax = plt.subplots(figsize=(6, 4))\n",
    "            x = np.arange(len(horizons))\n",
    "            ax.bar(x, values)\n",
    "\n",
    "            # Label each bar with its corresponding H\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([f\"{H}h\" for H in horizons])\n",
    "\n",
    "            ax.set_ylabel(metric_name)\n",
    "            ax.set_title(f\"{model_name} {metric_name} vs Horizon\")\n",
    "            ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "            # Add value text on each bar\n",
    "            for i, v in enumerate(values):\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    v,\n",
    "                    f\"{v:.3f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=9\n",
    "                )\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save file\n",
    "            save_path = model_dir / f\"{metric_name}_vs_H.png\"\n",
    "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def plot_loso_summary(summary, title=None, save_path=None):\n",
    "    metrics = list(summary.keys())\n",
    "    means = [summary[m][\"mean\"] for m in metrics]\n",
    "    stds  = [summary[m][\"std\"]  for m in metrics]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(x, means, yerr=stds, capsize=5)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, rotation=15)\n",
    "    ax.set_ylabel(\"Metric value\")\n",
    "    ax.set_title(title or \"LOSO summary (mean ¬± std)\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    # Expected Calibration Error (ECE)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_ids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = bin_ids == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        prop = np.mean(mask)\n",
    "        avg_conf = y_prob[mask].mean()\n",
    "        avg_acc = y_true[mask].mean()\n",
    "        ece += prop * abs(avg_acc - avg_conf)\n",
    "    return float(ece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6a69c-12c4-4a9d-8982-def597cf9872",
   "metadata": {},
   "source": [
    "# üå≥ Training Base Teacher Models (LightGBM)\n",
    "\n",
    "This section trains and evaluates the LightGBM teacher (or general) models, including classifier calibration, temperature regression, split management, and optional plotting.\n",
    "\n",
    "## Training & Evaluation Components\n",
    "\n",
    "- **train_base_model**  \n",
    "  Trains the base LightGBM models:  \n",
    "  - Fits an `LGBMClassifier` on the training set  \n",
    "  - Applies isotonic calibration using `CalibratedClassifierCV`  \n",
    "  - Trains an `LGBMRegressor` on train+cal data for temperature prediction  \n",
    "  Returns the calibrated classifier and regression model.\n",
    "\n",
    "- **evaluate_base_model**  \n",
    "  Evaluates a trained model on a test slice:  \n",
    "  - Computes AUROC, AUPRC, Brier Score, ECE, temperature MAE  \n",
    "  - Derives confusion matrix metrics (TP, FP, TN, FN)  \n",
    "  - Generates optional plots (ROC, PR, Reliability)  \n",
    "  - Returns a metrics dictionary and predicted probabilities.\n",
    "\n",
    "- **train_model**  \n",
    "  High-level wrapper that:  \n",
    "  - Builds features/targets using `build_xy`  \n",
    "  - Performs a 70/15/15 train‚Äìcal‚Äìtest split  \n",
    "  - Trains the base model using `train_base_model`  \n",
    "  - Evaluates performance using `evaluate_base_model`  \n",
    "  - Returns `(calib, reg, metrics, p_te, y_te)` preserving the original API.\n",
    "\n",
    "## Hyperparameters\n",
    "- `threshold` ‚Üí hard decision cutoff for frost classification  \n",
    "- `threads` ‚Üí number of CPU threads for LightGBM  \n",
    "- `verbose` ‚Üí enables/prints detailed output  \n",
    "- `plots` ‚Üí controls saving ROC/PR/reliability figures  \n",
    "\n",
    "## Key Outputs\n",
    "- Calibrated probability model (`calib`)  \n",
    "- Temperature regression model (`reg`)  \n",
    "- Teacher metrics (AUROC, AUPRC, Brier, ECE, MAE)  \n",
    "- Test-set probability predictions (`p_te`)  \n",
    "- Test frost labels (`y_te`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677a4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_model(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_cal,\n",
    "    y_cal,\n",
    "    X_trcal,\n",
    "    y_temp_trcal,\n",
    "    H=6,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the base (general/teacher) model:\n",
    "      - LGBMClassifier + isotonic calibration\n",
    "      - LGBMRegressor for temperature\n",
    "\n",
    "    This function does *no* evaluation or plotting.\n",
    "    \"\"\"\n",
    "    # --- Classifier ---\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.002,\n",
    "        class_weight=\"balanced\",\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        num_threads=threads,\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    calib = CalibratedClassifierCV(estimator=clf, method=\"isotonic\", cv=5)\n",
    "    calib.fit(X_cal, y_cal)\n",
    "\n",
    "    # --- Regressor (train on train+cal) ---\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        num_threads=threads,\n",
    "    )\n",
    "    reg.fit(X_trcal, y_temp_trcal)\n",
    "\n",
    "    return calib, reg\n",
    "\n",
    "\n",
    "def evaluate_base_model(\n",
    "    calib,\n",
    "    reg,\n",
    "    X_te,\n",
    "    y_te,\n",
    "    y_temp_te,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"general\",\n",
    "):\n",
    "    # classifier probs on test\n",
    "    p_te = calib.predict_proba(X_te)[:, 1]\n",
    "    y_pred = (p_te >= threshold).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr       = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    tnr       = tn / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    # regression preds on test\n",
    "    yhat_te = reg.predict(X_te)\n",
    "\n",
    "    ece = compute_ece(y_te.values, p_te, n_bins=10)\n",
    "\n",
    "    metrics = {\n",
    "        \"AUROC\": float(roc_auc_score(y_te, p_te)),\n",
    "        \"AUPRC\": float(average_precision_score(y_te, p_te)),\n",
    "        \"Brier\": float(brier_score_loss(y_te, p_te)),\n",
    "        \"ECE\": float(ece),\n",
    "        \"MAE_temp\": float(mean_absolute_error(y_temp_te, yhat_te)),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"TP\": int(tp),\n",
    "        \"FP\": int(fp),\n",
    "        \"TN\": int(tn),\n",
    "        \"FN\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TNR\": float(tnr),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n========== {name.upper()} MODEL EVALUATION (H = {H}h) ==========\")\n",
    "    \n",
    "        print(\"\\n--- Discrimination & Temp Regression ---\")\n",
    "        print(f\"AUROC:      {metrics['AUROC']:.4f}\")\n",
    "        print(f\"AUPRC:      {metrics['AUPRC']:.4f}\")\n",
    "        print(f\"MAE_temp:   {metrics['MAE_temp']:.4f}\")\n",
    "    \n",
    "        print(\"\\n--- Calibration Metrics ---\")\n",
    "        print(f\"Brier:      {metrics['Brier']:.4f}\")\n",
    "        print(f\"ECE:        {metrics['ECE']:.4f}\")\n",
    "    \n",
    "        print(\"\\n--- Classification Summary ---\")\n",
    "        print(\n",
    "            f\"thr={threshold:.2f} | \"\n",
    "            f\"TP={tp} FP={fp} TN={tn} FN={fn} \"\n",
    "            f\"(precision={precision:.3f}, recall={recall:.3f}, FPR={fpr:.3f})\"\n",
    "        )\n",
    "    \n",
    "        print(\n",
    "            f\"\\nNext-{H}h frost probability: {p_te[-1]*100:.2f}%\\n\"\n",
    "            f\"Predicted temperature:      {yhat_te[-1]:.2f} ¬∞C\"\n",
    "        )\n",
    "    \n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    if plots:\n",
    "        model_dir = PLOTS_ROOT / str(name)\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        title_suffix = f\"{name} (H={H}h)\"\n",
    "\n",
    "        prob_dict = {str(name): p_te}\n",
    "\n",
    "        plot_roc_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            title=f\"ROC {title_suffix}\",\n",
    "            save_path=model_dir / f\"roc_H{H}.png\",\n",
    "        )\n",
    "        plot_pr_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            title=f\"PR {title_suffix}\",\n",
    "            save_path=model_dir / f\"pr_H{H}.png\",\n",
    "        )\n",
    "        plot_reliability_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            n_bins=10,\n",
    "            title=f\"Reliability {title_suffix}\",\n",
    "            save_path=model_dir / f\"reliability_H{H}.png\",\n",
    "        )\n",
    "\n",
    "    return metrics, p_te\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    df,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"general\",\n",
    "):\n",
    "    # --- Build features/targets ---\n",
    "    X, y_frost, y_temp, feat_cols = build_xy(df, H)\n",
    "\n",
    "    n = len(X)\n",
    "    i_tr, i_cal = int(n * 0.70), int(n * 0.85)\n",
    "\n",
    "    X_tr, y_tr = X.iloc[:i_tr],        y_frost.iloc[:i_tr]\n",
    "    X_cal, y_cal = X.iloc[i_tr:i_cal], y_frost.iloc[i_tr:i_cal]\n",
    "    X_te,  y_te  = X.iloc[i_cal:],     y_frost.iloc[i_cal:]\n",
    "    y_temp_trcal = y_temp.iloc[:i_cal]\n",
    "    y_temp_te    = y_temp.iloc[i_cal:]\n",
    "\n",
    "    X_trcal = pd.concat([X_tr, X_cal])\n",
    "\n",
    "    # --- TRAIN ---\n",
    "    calib, reg = train_base_model(\n",
    "        X_tr=X_tr,\n",
    "        y_tr=y_tr,\n",
    "        X_cal=X_cal,\n",
    "        y_cal=y_cal,\n",
    "        X_trcal=X_trcal,\n",
    "        y_temp_trcal=y_temp_trcal,\n",
    "        H=H,\n",
    "        threads=threads,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # --- EVAL ---\n",
    "    metrics, p_te = evaluate_base_model(\n",
    "        calib=calib,\n",
    "        reg=reg,\n",
    "        X_te=X_te,\n",
    "        y_te=y_te,\n",
    "        y_temp_te=y_temp_te,\n",
    "        H=H,\n",
    "        threshold=threshold,\n",
    "        verbose=verbose,\n",
    "        plots=plots,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    return calib, reg, metrics, p_te, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fced36-f741-4c3c-a6bb-294e141f6874",
   "metadata": {},
   "source": [
    "# üßë‚Äçüéì Weighted Ensemble Knowledge Distillation (WEKD)\n",
    "\n",
    "This section uses all teacher models to train a student model via weighted ensemble distillation.\n",
    "\n",
    "## WEKD Components\n",
    "- **Teacher correlation penalty:**  \n",
    "  Less correlated teachers receive higher influence.\n",
    "- **Teacher accuracy weighting:**  \n",
    "  More accurate teachers receive higher weight.\n",
    "- **Softmax normalization** generates final teacher weights.\n",
    "- **Ensembled teacher probabilities** are used as KD targets.\n",
    "\n",
    "## Hyperparameters\n",
    "- `alpha` ‚Üí classification KD blend  \n",
    "- `beta` ‚Üí regression KD blend\n",
    "- `threshold` ‚Üí hard prediction threshold\n",
    "- `verbose` ‚Üí verbose outputs\n",
    "\n",
    "## Student Outputs\n",
    "- Trained KD classifier (`student_clf`)\n",
    "- Trained KD temperature regressor (`student_reg`)\n",
    "- Student metrics (AUROC, AUPRC, Brier, ECE, MAE)\n",
    "- Plots comparing teacher ensemble vs. student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef7b4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _compute_teacher_weights(teachers, X_ref, y_ref, eps=1e-6):\n",
    "    # Compute teacher weights using accuracy and correlation penalty.\n",
    "    n_teachers = len(teachers)\n",
    "    if n_teachers == 1:\n",
    "        return np.array([1.0], dtype=float)\n",
    "\n",
    "    preds = np.column_stack([t.predict_proba(X_ref)[:, 1] for t in teachers])\n",
    "    hard_preds = (preds >= 0.5).astype(int)\n",
    "    y_arr = y_ref.values.reshape(-1, 1)\n",
    "    acc = (hard_preds == y_arr).mean(axis=0)\n",
    "\n",
    "    corr_matrix = np.corrcoef(preds, rowvar=False)\n",
    "    np.fill_diagonal(corr_matrix, 0.0)\n",
    "    sum_sq_corr = np.sum(corr_matrix ** 2, axis=1)\n",
    "\n",
    "    score = acc / (sum_sq_corr + eps)\n",
    "    score_shifted = score - score.max()\n",
    "    exp_score = np.exp(score_shifted)\n",
    "    weights = exp_score / exp_score.sum()\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _ensemble_probs(teachers, X, weights=None):\n",
    "    preds = np.column_stack([t.predict_proba(X)[:, 1] for t in teachers])\n",
    "    if weights is None:\n",
    "        w = np.ones(preds.shape[1], dtype=float) / preds.shape[1]\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=float)\n",
    "        w = w / w.sum()\n",
    "    return preds @ w\n",
    "\n",
    "\n",
    "def _ensemble_reg_preds(regressors, X, weights=None):\n",
    "    preds = np.column_stack([r.predict(X) for r in regressors])\n",
    "    if weights is None:\n",
    "        w = np.ones(preds.shape[1], dtype=float) / preds.shape[1]\n",
    "    else:\n",
    "        w = np.asarray(weights, dtype=float)\n",
    "        w = w / w.sum()\n",
    "    return preds @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2c72e4-c99a-4d93-9026-fbe429088b66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_student_kd(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_cal,\n",
    "    y_cal,\n",
    "    X_trcal,\n",
    "    y_temp_trcal,\n",
    "    teachers,\n",
    "    reg_teachers,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    teacher_weights=None,\n",
    "    alpha=0.7,\n",
    "    beta=0.7,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "):\n",
    "    # compute teacher weights if needed (on calibration slice)\n",
    "    if teacher_weights is None:\n",
    "        teacher_weights = _compute_teacher_weights(teachers, X_cal, y_cal)\n",
    "        if verbose:\n",
    "            print(\"Teacher weights (acc / sum corr^2 ‚Üí softmax):\", teacher_weights)\n",
    "\n",
    "    # ---------- Classification KD ----------\n",
    "    p_teacher_tr = _ensemble_probs(teachers, X_tr, teacher_weights)\n",
    "\n",
    "    # KD targets: blend hard labels + teacher soft probs\n",
    "    y_tr_kd = alpha * y_tr.values + (1.0 - alpha) * p_teacher_tr\n",
    "\n",
    "    student_clf = lgb.LGBMRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.002,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        num_threads=threads,\n",
    "    )\n",
    "    student_clf.fit(X_tr, y_tr_kd)\n",
    "\n",
    "    # ---------- Regression KD ----------\n",
    "    temp_teacher_trcal = _ensemble_reg_preds(reg_teachers, X_trcal, teacher_weights)\n",
    "    y_temp_trcal_kd = beta * y_temp_trcal.values + (1.0 - beta) * temp_teacher_trcal\n",
    "\n",
    "    student_reg = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.04,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        num_threads=threads,\n",
    "    )\n",
    "    student_reg.fit(X_trcal, y_temp_trcal_kd)\n",
    "\n",
    "    return student_clf, student_reg, teacher_weights\n",
    "\n",
    "\n",
    "def evaluate_student_kd(\n",
    "    student_clf,\n",
    "    student_reg,\n",
    "    teachers,\n",
    "    reg_teachers,\n",
    "    teacher_weights,\n",
    "    X_te,\n",
    "    y_te,\n",
    "    y_temp_te,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"student\",\n",
    "):\n",
    "    # teacher ensemble probs on test\n",
    "    p_teacher_te = _ensemble_probs(teachers, X_te, teacher_weights)\n",
    "\n",
    "    # student probs on test\n",
    "    p_te_student = student_clf.predict(X_te)\n",
    "    p_te_student = np.clip(p_te_student, 0.0, 1.0)\n",
    "    y_pred = (p_te_student >= threshold).astype(int)\n",
    "\n",
    "    # regression preds on test\n",
    "    yhat_te = student_reg.predict(X_te)\n",
    "\n",
    "    # metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    fpr       = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    tnr       = tn / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    ece_student = compute_ece(y_te.values, p_te_student, n_bins=10)\n",
    "    ece_teacher = compute_ece(y_te.values, p_teacher_te, n_bins=10)\n",
    "\n",
    "    metrics = {\n",
    "        \"AUROC\":    float(roc_auc_score(y_te, p_te_student)),\n",
    "        \"AUPRC\":    float(average_precision_score(y_te, p_te_student)),\n",
    "        \"Brier\":    float(brier_score_loss(y_te, p_te_student)),\n",
    "        \"ECE\":      float(ece_student),\n",
    "        \"MAE_temp\": float(mean_absolute_error(y_temp_te, yhat_te)),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"TP\": int(tp),\n",
    "        \"FP\": int(fp),\n",
    "        \"TN\": int(tn),\n",
    "        \"FN\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TNR\": float(tnr),\n",
    "        \"teacher_weights\": teacher_weights.tolist(),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n========== STUDENT MODEL EVALUATION (H = {H}h) ==========\")\n",
    "    \n",
    "        print(\"\\n--- Discrimination & Temp Regression ---\")\n",
    "        print(f\"AUROC:      {metrics['AUROC']:.4f}\")\n",
    "        print(f\"AUPRC:      {metrics['AUPRC']:.4f}\")\n",
    "        print(f\"MAE_temp:   {metrics['MAE_temp']:.4f}\")\n",
    "    \n",
    "        print(\"\\n--- Calibration Metrics ---\")\n",
    "        print(f\"Brier:      {metrics['Brier']:.4f}\")\n",
    "        print(f\"ECE:        {metrics['ECE']:.4f}\")\n",
    "    \n",
    "        print(\"\\n--- Classification Summary ---\")\n",
    "        print(\n",
    "            f\"thr={threshold:.2f} | \"\n",
    "            f\"TP={tp} FP={fp} TN={tn} FN={fn} \"\n",
    "            f\"(precision={precision:.3f}, recall={recall:.3f}, FPR={fpr:.3f})\"\n",
    "        )\n",
    "    \n",
    "        print(\n",
    "            f\"\\nNext-{H}h frost probability: {p_te_student[-1]*100:.2f}%\\n\"\n",
    "            f\"Predicted temperature:      {yhat_te[-1]:.2f} ¬∞C\"\n",
    "        )\n",
    "\n",
    "        print(\"=========================================================\\n\")\n",
    "\n",
    "    if plots:\n",
    "        model_dir = PLOTS_ROOT / str(name)\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        title_suffix = f\"{name} (H={H}h)\"\n",
    "\n",
    "        prob_dict = {\n",
    "            \"TeacherEnsemble\": p_teacher_te,\n",
    "            \"Student\": p_te_student,\n",
    "        }\n",
    "\n",
    "        plot_roc_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            title=f\"ROC {title_suffix}\",\n",
    "            save_path=model_dir / f\"roc_H{H}.png\",\n",
    "        )\n",
    "        plot_pr_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            title=f\"PR {title_suffix}\",\n",
    "            save_path=model_dir / f\"pr_H{H}.png\",\n",
    "        )\n",
    "        plot_reliability_curves(\n",
    "            y_te,\n",
    "            prob_dict,\n",
    "            n_bins=10,\n",
    "            title=f\"Reliability {title_suffix}\",\n",
    "            save_path=model_dir / f\"reliability_H{H}.png\",\n",
    "        )\n",
    "\n",
    "        auprc_teacher = float(average_precision_score(y_te, p_teacher_te))\n",
    "        brier_teacher = float(brier_score_loss(y_te, p_teacher_te))\n",
    "\n",
    "        plot_metric_bar(\n",
    "            \"AUPRC\",\n",
    "            {\"TeacherEnsemble\": auprc_teacher, \"Student\": metrics[\"AUPRC\"]},\n",
    "            title=f\"AUPRC {title_suffix}\",\n",
    "            save_path=model_dir / f\"auprc_H{H}.png\",\n",
    "        )\n",
    "        plot_metric_bar(\n",
    "            \"Brier\",\n",
    "            {\"TeacherEnsemble\": brier_teacher, \"Student\": metrics[\"Brier\"]},\n",
    "            title=f\"Brier {title_suffix}\",\n",
    "            save_path=model_dir / f\"brier_H{H}.png\",\n",
    "        )\n",
    "        plot_metric_bar(\n",
    "            \"ECE\",\n",
    "            {\"TeacherEnsemble\": ece_teacher, \"Student\": metrics[\"ECE\"]},\n",
    "            title=f\"ECE {title_suffix}\",\n",
    "            save_path=model_dir / f\"ece_H{H}.png\",\n",
    "        )\n",
    "\n",
    "    return metrics, p_te_student, p_teacher_te\n",
    "\n",
    "\n",
    "def train_student_from_ensemble(\n",
    "    df,\n",
    "    teachers,\n",
    "    reg_teachers,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    teacher_weights=None,\n",
    "    alpha=0.7,\n",
    "    beta=0.7,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"student\",\n",
    "):\n",
    "    # --- Build features/targets ---\n",
    "    X, y_frost, y_temp, feat_cols = build_xy(df, H)\n",
    "\n",
    "    n = len(X)\n",
    "    i_tr, i_cal = int(n * 0.70), int(n * 0.85)\n",
    "\n",
    "    X_tr, y_tr = X.iloc[:i_tr],        y_frost.iloc[:i_tr]\n",
    "    X_cal, y_cal = X.iloc[i_tr:i_cal], y_frost.iloc[i_tr:i_cal]\n",
    "    X_te,  y_te  = X.iloc[i_cal:],     y_frost.iloc[i_cal:]\n",
    "    y_temp_trcal = y_temp.iloc[:i_cal]\n",
    "    y_temp_te    = y_temp.iloc[i_cal:]\n",
    "\n",
    "    X_trcal = pd.concat([X_tr, X_cal])\n",
    "\n",
    "    # --- TRAIN ---\n",
    "    student_clf, student_reg, teacher_weights = train_student_kd(\n",
    "        X_tr=X_tr,\n",
    "        y_tr=y_tr,\n",
    "        X_cal=X_cal,\n",
    "        y_cal=y_cal,\n",
    "        X_trcal=X_trcal,\n",
    "        y_temp_trcal=y_temp_trcal,\n",
    "        teachers=teachers,\n",
    "        reg_teachers=reg_teachers,\n",
    "        H=H,\n",
    "        threshold=threshold,\n",
    "        teacher_weights=teacher_weights,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        threads=threads,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # --- EVAL ---\n",
    "    metrics, p_te_student, p_teacher_te = evaluate_student_kd(\n",
    "        student_clf=student_clf,\n",
    "        student_reg=student_reg,\n",
    "        teachers=teachers,\n",
    "        reg_teachers=reg_teachers,\n",
    "        teacher_weights=teacher_weights,\n",
    "        X_te=X_te,\n",
    "        y_te=y_te,\n",
    "        y_temp_te=y_temp_te,\n",
    "        H=H,\n",
    "        threshold=threshold,\n",
    "        verbose=verbose,\n",
    "        plots=plots,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    return student_clf, student_reg, metrics, p_te_student, p_teacher_te, y_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd201324-686b-4aab-99b0-9f7c96f0745f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üöÄ Main Pipeline: General & Student Model Execution\n",
    "\n",
    "This final section executes the full training procedure.\n",
    "\n",
    "## Pipeline Steps\n",
    "- Train a general LightGBM model on all stations combined\n",
    "- Train all teacher models per station\n",
    "- Train student model using Weighted Ensemble Knowledge Distillation (WEKD)\n",
    "- Save all models and plots\n",
    "- Clean RAM between horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fff207-e929-4d93-8278-e639ae0a952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Clear Model and Plot Directory\n",
    "# !rm -rf models/*\n",
    "# !rm -rf plots/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6bbd4b-21b1-4b30-a339-74e50adbbd80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "datasets = clean_datasets(get_datasets()) # Dictionary of CIMIS Datasets\n",
    "stations = list(datasets.keys()) # Station Names\n",
    "df_all = pd.concat(list(datasets.values()), ignore_index=True) # DataFrame with All Stations Data\n",
    "\n",
    "metrics_over_H = {}  # {model_name: {H: metrics_dict}}\n",
    "\n",
    "# HyperParams\n",
    "tfs = [3, 6, 12, 24] # TimeFrames\n",
    "threshold = 0.3 # Threshold for Models\n",
    "thread_per_model = 16 # Threads Per Model\n",
    "metrics_to_plot = [\"Brier\", \"ECE\", \"AUPRC\", \"AUROC\"]  # Metrics to Plot\n",
    "verbose = True # Verbose Output\n",
    "save_models = True # Save Models\n",
    "save_plots = True # Save Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c5209-30cb-4031-a8dd-d8b0104238c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENERAL_DIR = MODELS_ROOT / \"general\"\n",
    "GENERAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Track metrics across horizons\n",
    "if \"general\" not in metrics_over_H:\n",
    "    metrics_over_H[\"general\"] = {}\n",
    "\n",
    "print(f\"\\n Training General Model...\")\n",
    "for tf in tfs:\n",
    "    calib, reg, general_metrics, p_te_gen, y_te_gen = train_model(\n",
    "        df=df_all, \n",
    "        H=tf, \n",
    "        threshold=threshold,\n",
    "        threads=thread_per_model, \n",
    "        verbose=verbose,\n",
    "        plots=save_plots,        \n",
    "        name=\"general\"\n",
    "    )\n",
    "\n",
    "    general_calib_path = GENERAL_DIR / f\"H{tf}_calib.joblib\"\n",
    "    general_reg_path   = GENERAL_DIR / f\"H{tf}_reg.joblib\"\n",
    "\n",
    "    metrics_over_H[\"general\"][tf] = general_metrics\n",
    "\n",
    "    if save_models:\n",
    "        joblib.dump(calib, general_calib_path)\n",
    "        joblib.dump(reg,   general_reg_path)\n",
    "\n",
    "    # Delete All References and Garbage Collect [Memory Clean Up]\n",
    "    del calib, reg\n",
    "    gc.collect()\n",
    "\n",
    "if save_plots:\n",
    "    plot_horizon_summaries(metrics_over_H, metrics_to_plot)\n",
    "    metrics_over_H = {} # Reset Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416b1a-1da2-4fdc-94b1-e870b97d78eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Directories for Teachers\n",
    "TEACHERS_DIR = {}\n",
    "for station in stations:\n",
    "    station_dir = MODELS_ROOT / station\n",
    "    station_dir.mkdir(parents=True, exist_ok=True)\n",
    "    TEACHERS_DIR[station] = station_dir\n",
    "    \n",
    "# Create Directory for Student\n",
    "STUDENTS_DIR = MODELS_ROOT / \"students\"\n",
    "STUDENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tf in tfs:\n",
    "    # Ensemble of Teachers\n",
    "    teachers = []\n",
    "    reg_teachers = []\n",
    "\n",
    "    # Train, Test, Plot, then Dump Teacher Models\n",
    "    for station in stations:\n",
    "        print(f\"\\nTraining {station} (H={tf})...\")\n",
    "\n",
    "        calib, reg, metrics, p_te, y_te = train_model(\n",
    "            df=datasets[station],\n",
    "            H=tf,\n",
    "            threshold=threshold,\n",
    "            threads=thread_per_model,\n",
    "            verbose=verbose,\n",
    "            plots=save_plots,                   \n",
    "            name=station,                 \n",
    "        )\n",
    "\n",
    "        teachers.append(calib)\n",
    "        reg_teachers.append(reg)\n",
    "\n",
    "        # Track metrics across horizons\n",
    "        if station not in metrics_over_H:\n",
    "            metrics_over_H[station] = {}\n",
    "        metrics_over_H[station][tf] = metrics\n",
    "\n",
    "        if save_models:\n",
    "            joblib.dump(calib, TEACHERS_DIR[station] / f\"H{tf}_calib.joblib\")\n",
    "            joblib.dump(reg,   TEACHERS_DIR[station] / f\"H{tf}_reg.joblib\")\n",
    "\n",
    "        del p_te, y_te\n",
    "\n",
    "    # Train, Test, Plot, then Dump Student Model\n",
    "    print(f\"\\nTraining Student from Teachers (H={tf})...\")\n",
    "    \n",
    "    student_clf, student_reg, student_metrics, p_te_student, p_teacher_te, y_te_student = train_student_from_ensemble(\n",
    "        df=df_all,\n",
    "        teachers=teachers,\n",
    "        reg_teachers=reg_teachers,\n",
    "        H=tf,\n",
    "        threshold=threshold,\n",
    "        threads=thread_per_model,\n",
    "        alpha=0.7,\n",
    "        beta=0.7,\n",
    "        verbose=verbose,\n",
    "        plots=save_plots,    \n",
    "        name=\"student\",\n",
    "    )\n",
    "\n",
    "    # Track metrics across horizons\n",
    "    if \"student\" not in metrics_over_H:\n",
    "        metrics_over_H[\"student\"] = {}\n",
    "    metrics_over_H[\"student\"][tf] = student_metrics\n",
    "    \n",
    "    if save_models:\n",
    "        joblib.dump(student_clf, STUDENTS_DIR / f\"H{tf}_calib.joblib\")\n",
    "        joblib.dump(student_reg, STUDENTS_DIR / f\"H{tf}_reg.joblib\")\n",
    "\n",
    "    # Delete All References and Garbage Collect [Memory Clean Up]\n",
    "    del teachers, reg_teachers\n",
    "    del student_clf, student_reg, p_te_student, p_teacher_te, y_te_student\n",
    "    gc.collect()\n",
    "\n",
    "if save_plots:\n",
    "    plot_horizon_summaries(metrics_over_H, metrics_to_plot)\n",
    "    metrics_over_H = {} # Reset Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c215b0-60bf-4de0-9df5-0c86afe81fdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üìç Spatial Generalization with Leave-One-Station-Out (LOSO)\n",
    "\n",
    "Leave-One-Station-Out (LOSO) evaluates how well models generalize to **unseen geographic locations** by repeatedly holding out one station for testing while training on all others.\n",
    "\n",
    "## What LOSO Does\n",
    "- Select one station as the held-out test set  \n",
    "- Train on all remaining stations  \n",
    "- Evaluate only on the held-out station  \n",
    "- Repeat for every station  \n",
    "\n",
    "**Outputs:**  \n",
    "Per-station metrics, performance distributions, and mean ¬± std summaries showing spatial robustness.\n",
    "\n",
    "## Why LOSO Matters\n",
    "Random or temporal splits cannot detect spatial overfitting. LOSO answers:\n",
    "- Does the model memorize station-specific microclimates?  \n",
    "- Can it generalize to unseen locations?  \n",
    "- Does knowledge distillation improve spatial transfer?  \n",
    "- How does the general model compare to the student across geography?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c21e18-e3a6-4960-9fe3-1b06be9f158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loso_general(\n",
    "    datasets,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"general\",\n",
    "):\n",
    "    stations = list(datasets.keys())\n",
    "    per_station_metrics = {}\n",
    "\n",
    "    loso_dir = PLOTS_ROOT / \"LOSO\" / f\"{name}_H{H}\"\n",
    "    if plots:\n",
    "        loso_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # LOOP OVER STATIONS (Leave-One-Station-Out)\n",
    "    for held_out in stations:\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"   LOSO GENERAL: Held-out station '{held_out}' (H={H})\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "        # Build LOSO training & test sets\n",
    "        df_train = pd.concat(\n",
    "            [df for s, df in datasets.items() if s != held_out],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_test = datasets[held_out]\n",
    "\n",
    "        # Build XY for train split\n",
    "        X_all, y_frost_all, y_temp_all, _ = build_xy(df_train, H)\n",
    "\n",
    "        n_all = len(X_all)\n",
    "        i_tr, i_cal = int(0.70 * n_all), int(0.85 * n_all)\n",
    "\n",
    "        X_tr, y_tr       = X_all.iloc[:i_tr],       y_frost_all.iloc[:i_tr]\n",
    "        X_cal, y_cal     = X_all.iloc[i_tr:i_cal],  y_frost_all.iloc[i_tr:i_cal]\n",
    "        X_te_train       = X_all.iloc[i_cal:]\n",
    "        y_te_train       = y_frost_all.iloc[i_cal:]\n",
    "        y_temp_trcal     = y_temp_all.iloc[:i_cal]\n",
    "        y_temp_te_train  = y_temp_all.iloc[i_cal:]\n",
    "        X_trcal = pd.concat([X_tr, X_cal])\n",
    "\n",
    "        # Train base general model on df_train only\n",
    "        calib, reg = train_base_model(\n",
    "            X_tr=X_tr,\n",
    "            y_tr=y_tr,\n",
    "            X_cal=X_cal,\n",
    "            y_cal=y_cal,\n",
    "            X_trcal=X_trcal,\n",
    "            y_temp_trcal=y_temp_trcal,\n",
    "            H=H,\n",
    "            threads=threads,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # Evaluate on HELD-OUT station\n",
    "        X_te, y_frost_te, y_temp_te, _ = build_xy(df_test, H)\n",
    "\n",
    "        metrics, p_te = evaluate_base_model(\n",
    "            calib=calib,\n",
    "            reg=reg,\n",
    "            X_te=X_te,\n",
    "            y_te=y_frost_te,\n",
    "            y_temp_te=y_temp_te,\n",
    "            H=H,\n",
    "            threshold=threshold,\n",
    "            verbose=verbose,\n",
    "            plots=False,  # avoid spam\n",
    "            name=f\"LOSO_general_excluding_{held_out}\",\n",
    "        )\n",
    "\n",
    "        # Save metrics for this station\n",
    "        per_station_metrics[held_out] = metrics\n",
    "\n",
    "        # Drop big dataframes and arrays\n",
    "        del df_train, df_test\n",
    "        del X_all, y_frost_all, y_temp_all\n",
    "        del X_tr, y_tr, X_cal, y_cal, X_te_train, y_te_train\n",
    "        del y_temp_trcal, y_temp_te_train, X_trcal\n",
    "        del X_te, y_frost_te, y_temp_te\n",
    "        # Drop models and predictions for this fold\n",
    "        del calib, reg, p_te\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute LOSO SUMMARY (mean ¬± std) in two metric groups\n",
    "    group_A = [\"AUROC\", \"AUPRC\", \"MAE_temp\"]   # discrimination & regression\n",
    "    group_B = [\"Brier\", \"ECE\"]                 # calibration metrics\n",
    "\n",
    "    summary = {\"group_A\": {}, \"group_B\": {}}\n",
    "\n",
    "    for m in group_A:\n",
    "        vals = np.array([per_station_metrics[s][m] for s in stations], dtype=float)\n",
    "        summary[\"group_A\"][m] = {\n",
    "            \"mean\": float(vals.mean()),\n",
    "            \"std\":  float(vals.std()),\n",
    "        }\n",
    "\n",
    "    for m in group_B:\n",
    "        vals = np.array([per_station_metrics[s][m] for s in stations], dtype=float)\n",
    "        summary[\"group_B\"][m] = {\n",
    "            \"mean\": float(vals.mean()),\n",
    "            \"std\":  float(vals.std()),\n",
    "        }\n",
    "\n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n========== LOSO GENERAL SUMMARY (Mean ¬± Std) ==========\")\n",
    "\n",
    "        print(\"\\n--- Discrimination & Temp Regression ---\")\n",
    "        for m in group_A:\n",
    "            print(f\"{m}: {summary['group_A'][m]['mean']:.4f} ¬± {summary['group_A'][m]['std']:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Calibration Metrics ---\")\n",
    "        for m in group_B:\n",
    "            print(f\"{m}: {summary['group_B'][m]['mean']:.4f} ¬± {summary['group_B'][m]['std']:.4f}\")\n",
    "\n",
    "        print(\"=======================================================\\n\")\n",
    "\n",
    "    # Summary plots\n",
    "    if plots:\n",
    "        # Main summary: AUROC, AUPRC, MAE_temp\n",
    "        plot_loso_summary(\n",
    "            summary[\"group_A\"],\n",
    "            title=f\"LOSO General Model Summary ‚Äì Discrimination & Temp (H={H})\",\n",
    "            save_path=loso_dir / f\"summary_LOSO_general_main_H{H}.png\",\n",
    "        )\n",
    "\n",
    "        # Calibration-only summary: Brier, ECE\n",
    "        plot_loso_summary(\n",
    "            summary[\"group_B\"],\n",
    "            title=f\"LOSO General Model Summary ‚Äì Calibration (H={H})\",\n",
    "            save_path=loso_dir / f\"summary_LOSO_general_calib_H{H}.png\",\n",
    "        )\n",
    "\n",
    "    return per_station_metrics, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4654f805-53c5-4b72-a54a-3dca43d85c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loso_student(\n",
    "    datasets,\n",
    "    teachers_by_station,\n",
    "    reg_teachers_by_station,\n",
    "    H=6,\n",
    "    threshold=0.5,\n",
    "    alpha=0.7,\n",
    "    beta=0.7,\n",
    "    threads=1,\n",
    "    verbose=True,\n",
    "    plots=False,\n",
    "    name=\"student\",\n",
    "):\n",
    "    stations = list(datasets.keys())\n",
    "    per_station_metrics = {}\n",
    "\n",
    "    # Where to save LOSO summaries and plots\n",
    "    loso_dir =  PLOTS_ROOT / \"LOSO\" / f\"{name}_H{H}\"\n",
    "    if plots:\n",
    "        loso_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for held_out in stations:\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"   LOSO Fold ‚Äì Held-out station: {held_out} (H={H})\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "        # Build LOSO training data\n",
    "        df_train = pd.concat(\n",
    "            [df for s, df in datasets.items() if s != held_out],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        df_test = datasets[held_out]\n",
    "\n",
    "        # Teachers used for this fold = all except held-out\n",
    "        fold_stations = [s for s in stations if s != held_out]\n",
    "        fold_teachers     = [teachers_by_station[s] for s in fold_stations]\n",
    "        fold_reg_teachers = [reg_teachers_by_station[s] for s in fold_stations]\n",
    "\n",
    "        # Build XY for the TRAIN dataset (for computing teacher weights)\n",
    "        X_all, y_frost_all, y_temp_all, _ = build_xy(df_train, H)\n",
    "        n_all = len(X_all)\n",
    "        i_tr, i_cal = int(0.70 * n_all), int(0.85 * n_all)\n",
    "        X_tr, y_tr   = X_all.iloc[:i_tr],       y_frost_all.iloc[:i_tr]\n",
    "        X_cal, y_cal = X_all.iloc[i_tr:i_cal], y_frost_all.iloc[i_tr:i_cal]\n",
    "        X_te_train   = X_all.iloc[i_cal:]\n",
    "        y_te_train   = y_frost_all.iloc[i_cal:]\n",
    "        y_temp_trcal = y_temp_all.iloc[:i_cal]\n",
    "        y_temp_te_train = y_temp_all.iloc[i_cal:]\n",
    "        X_trcal = pd.concat([X_tr, X_cal])\n",
    "\n",
    "        # Compute teacher weights for this LOSO fold\n",
    "        teacher_weights = _compute_teacher_weights(fold_teachers, X_cal, y_cal)\n",
    "\n",
    "        # Train a WEKD student on df_train only\n",
    "        student_clf, student_reg, teacher_weights = train_student_kd(\n",
    "            X_tr=X_tr,\n",
    "            y_tr=y_tr,\n",
    "            X_cal=X_cal,\n",
    "            y_cal=y_cal,\n",
    "            X_trcal=X_trcal,\n",
    "            y_temp_trcal=y_temp_trcal,\n",
    "            teachers=fold_teachers,\n",
    "            reg_teachers=fold_reg_teachers,\n",
    "            H=H,\n",
    "            threshold=threshold,\n",
    "            teacher_weights=teacher_weights,\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            threads=threads,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # Evaluate student on HELD-OUT station only\n",
    "        X_te, y_frost_te, y_temp_te, _ = build_xy(df_test, H)\n",
    "\n",
    "        metrics, p_student, p_teacher = evaluate_student_kd(\n",
    "            student_clf=student_clf,\n",
    "            student_reg=student_reg,\n",
    "            teachers=fold_teachers,\n",
    "            reg_teachers=fold_reg_teachers,\n",
    "            teacher_weights=teacher_weights,\n",
    "            X_te=X_te,\n",
    "            y_te=y_frost_te,\n",
    "            y_temp_te=y_temp_te,\n",
    "            H=H,\n",
    "            threshold=threshold,\n",
    "            verbose=verbose,\n",
    "            plots=False,   # avoid too many plots during LOSO\n",
    "            name=f\"LOSO_student_excluding_{held_out}\",\n",
    "        )\n",
    "\n",
    "        # Save per-station metrics\n",
    "        per_station_metrics[held_out] = metrics\n",
    "\n",
    "        # Big dataframes and arrays\n",
    "        del df_train, df_test\n",
    "        del X_all, y_frost_all, y_temp_all\n",
    "        del X_tr, y_tr, X_cal, y_cal, X_te_train, y_te_train\n",
    "        del y_temp_trcal, y_temp_te_train, X_trcal\n",
    "        del X_te, y_frost_te, y_temp_te\n",
    "        # Models and predictions\n",
    "        del student_clf, student_reg\n",
    "        del p_student, p_teacher\n",
    "        del fold_teachers, fold_reg_teachers\n",
    "        del teacher_weights\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # Metric groups (due to different numeric scales)\n",
    "    group_A = [\"AUROC\", \"AUPRC\", \"MAE_temp\"]   # discrimination & regression\n",
    "    group_B = [\"Brier\", \"ECE\"]                 # calibration metrics\n",
    "    \n",
    "    summary = {\"group_A\": {}, \"group_B\": {}}\n",
    "    \n",
    "    # Compute per-group means + std\n",
    "    for m in group_A:\n",
    "        vals = np.array([per_station_metrics[s][m] for s in stations], dtype=float)\n",
    "        summary[\"group_A\"][m] = {\n",
    "            \"mean\": float(vals.mean()),\n",
    "            \"std\":  float(vals.std()),\n",
    "        }\n",
    "    \n",
    "    for m in group_B:\n",
    "        vals = np.array([per_station_metrics[s][m] for s in stations], dtype=float)\n",
    "        summary[\"group_B\"][m] = {\n",
    "            \"mean\": float(vals.mean()),\n",
    "            \"std\":  float(vals.std()),\n",
    "        }\n",
    "    \n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(\"\\n\\n========== LOSO SUMMARY (Mean ¬± Std) ==========\")\n",
    "    \n",
    "        print(\"\\n--- Discrimination & Temp Regression ---\")\n",
    "        for m in group_A:\n",
    "            print(f\"{m}: {summary['group_A'][m]['mean']:.4f} ¬± {summary['group_A'][m]['std']:.4f}\")\n",
    "    \n",
    "        print(\"\\n--- Calibration Metrics ---\")\n",
    "        for m in group_B:\n",
    "            print(f\"{m}: {summary['group_B'][m]['mean']:.4f} ¬± {summary['group_B'][m]['std']:.4f}\")\n",
    "    \n",
    "        print(\"==============================================\\n\")\n",
    "    \n",
    "    # Summary plots\n",
    "    if plots:\n",
    "        # Main summary: discrimination & temp regression (AUROC, AUPRC, MAE_temp)\n",
    "        plot_loso_summary(\n",
    "            summary[\"group_A\"],\n",
    "            title=f\"LOSO WEKD Student Summary ‚Äì Discrimination & Temp (H={H})\",\n",
    "            save_path=loso_dir / f\"summary_LOSO_main_H{H}.png\",\n",
    "        )\n",
    "    \n",
    "        # Calibration-only summary: Brier, ECE\n",
    "        plot_loso_summary(\n",
    "            summary[\"group_B\"],\n",
    "            title=f\"LOSO WEKD Student Summary ‚Äì Calibration (H={H})\",\n",
    "            save_path=loso_dir / f\"summary_LOSO_calib_H{H}.png\",\n",
    "        )\n",
    "\n",
    "    return per_station_metrics, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617797cd-ea0b-4695-b7a3-084c3201b9f0",
   "metadata": {},
   "source": [
    "# üåç LOSO Pipeline: Spatial Generalization\n",
    "\n",
    "The LOSO pipeline tests how models generalize to unseen stations by training on all but one station and evaluating on the held-out location. This process repeats for every station.\n",
    "\n",
    "## Pipwline Steps\n",
    "- Hold out one station as test data  \n",
    "- Train on all remaining stations  \n",
    "- Evaluate only on the held-out station  \n",
    "- Repeat for all stations  \n",
    "- Compute mean ¬± std across metrics  \n",
    "- Produce two plots:  \n",
    "  - Main (AUROC, AUPRC, MAE_temp)  \n",
    "  - Calibration (Brier, ECE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a758e0-1eb1-433a-a21b-6a9ac6fb8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tf in tfs:\n",
    "    run_loso_general(\n",
    "        datasets,\n",
    "        H=tf,\n",
    "        threshold=threshold,\n",
    "        threads=thread_per_model,\n",
    "        verbose=verbose,\n",
    "        plots=save_plots, # makes LOSO summary plot with mean ¬± std\n",
    "        name=\"general\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3ef5-18af-4366-8e47-43fff5d439c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store LOSO results for later analysis / tables\n",
    "loso_student_results = {}  # H -> {\"per_station\": ..., \"summary\": ...}\n",
    "\n",
    "for tf in tfs:\n",
    "    print(f\"\\n=== Running LOSO for student (H={tf}) ===\")\n",
    "\n",
    "    # Load teachers for this horizon\n",
    "    teachers_by_station = {}\n",
    "    reg_teachers_by_station = {}\n",
    "\n",
    "    for station in stations:\n",
    "        calib_path = TEACHERS_DIR[station] / f\"H{tf}_calib.joblib\"\n",
    "        reg_path   = TEACHERS_DIR[station] / f\"H{tf}_reg.joblib\"\n",
    "\n",
    "        teachers_by_station[station]     = joblib.load(calib_path)\n",
    "        reg_teachers_by_station[station] = joblib.load(reg_path)\n",
    "\n",
    "    # Run LOSO for this horizon\n",
    "    per_station_metrics, summary = run_loso_student(\n",
    "        datasets=datasets,\n",
    "        teachers_by_station=teachers_by_station,\n",
    "        reg_teachers_by_station=reg_teachers_by_station,\n",
    "        H=tf,\n",
    "        threshold=threshold,\n",
    "        alpha=0.7,\n",
    "        beta=0.7,\n",
    "        threads=thread_per_model,\n",
    "        verbose=verbose,\n",
    "        plots=save_plots,  # makes LOSO summary plot with mean ¬± std\n",
    "        name=f\"student\",\n",
    "    )\n",
    "\n",
    "    # Save results by horizon\n",
    "    loso_student_results[tf] = {\n",
    "        \"per_station\": per_station_metrics,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "\n",
    "    # Memory Cleanup (safe version)\n",
    "\n",
    "    # Remove per-station teachers to prevent accumulation\n",
    "    del teachers_by_station\n",
    "    del reg_teachers_by_station\n",
    "\n",
    "    # Remove LOSO metrics dicts from RAM if you want\n",
    "    # (but KEEP loso_student_results ‚Äî that‚Äôs your output)\n",
    "    # del per_station_metrics\n",
    "    # del summary\n",
    "\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
